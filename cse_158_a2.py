# -*- coding: utf-8 -*-
"""CSE 158 A2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10MCizo5ynsGDzkpwdDDETinnO0Jzz2JL
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import random

from collections import defaultdict
from sklearn import metrics, linear_model
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR

# don't have to keep running it
from google.colab import drive
drive.mount('/content/drive')

"""# Setup
#### Read in data from Google Drive, convert to list of dictionaries
"""

# read in data located in root directory of Drive
df = pd.read_csv('/content/drive/My Drive/data/tomslee_airbnb_paris_1478_2017-07-25.csv')

# convert dataframe into a list of dictionaries, but probably do this after cleaning
#data = df.to_dict('records')
#data[0]

"""# Exploratory Data Analysis

## Basic Stats
"""

len(df), df.columns

df.head()

df.describe()

"""### Null Values

Clean up and explore dataframe before converting to list of dicts
"""

# count null values
df.isnull().sum()

"""Country and borough columns are completely empty. Since we know our data only comes from Paris, we can drop these whole columns."""

df = df.drop(columns = ['country', 'borough'])
df.isnull().sum()

"""weird that bathroom counts are all null, wonder if it'll make a difference; ignoring for now

what is minstay? min number of nights? i assume it's just 0? -> yes it is the minimum number of nights, usually its at least 1, some places its 2 or 3 kinda depends.

its kinda weird that bedrooms is 0? some of them are 0 because they are studios, there should definitely be some non-zero though
"""

df.head()

"""### Value_counts"""

df['city'].value_counts()

df['room_type'].value_counts()

df['neighborhood'].value_counts()

# number of reviews a listing has gotten
df['reviews'].value_counts()

df['overall_satisfaction'].value_counts()

df['accommodates'].value_counts()

df['bedrooms'].value_counts()

df['property_type'].value_counts()

"""### Correlation matrix"""

f = plt.figure(figsize=(19, 15))
plt.matshow(df.corr(), fignum=f.number)
plt.xticks(range(df.shape[1]), df.columns, fontsize=14, rotation=90)
plt.yticks(range(df.shape[1]), df.columns, fontsize=14)
cb = plt.colorbar()
cb.ax.tick_params(labelsize=14)
#plt.title('Correlation Matrix', fontsize=16);

"""## Basic Stats: Prices"""

len(df[df['price'] == 0])

df['price'].describe()

# searching for outliers
for i in [200, 500, 750, 1000, 1500, 2000, 3000, 4000, 5000, 7000]:
  subset = df[df['price'] <= i]
  print('i =', i)
  print('percentage of df:', 100 * len(subset)/len(df))
  print('num outside range from 0 to ' + str(i) + ':', len(df) - len(subset))
  print('---')

ax = sns.displot(df['price'], kind='kde')
#ax.set_title('Density Plot of Prices with Outliers')
axes = ax.axes.flatten()
#axes.set_title('Density Plot of Prices with Outliers')

small_prices = df[df['price'] <= 1500]

#ax = sns.distplot(small_prices['price']) -> switched to displot instead of distplot 
ax = sns.displot(small_prices['price'], kind='kde')
#data.xlim(0, 2000)

very_smol = df[df['price'] <= 500]
ax = sns.displot(very_smol['price'], kind='kde')

# mean of the prices and the subsets of prices

# full dataset mean
mean1 = df['price'].mean()
print('Mean of full dataset:', mean1)
print('----')

for i in [500, 750, 1000, 1500, 2000, 3000, 5000, 7000]:
  mean = df[df['price'] <= i]['price'].mean()
  print('Price maximum =', i)
  print('Num outliers removed:', len(df) - len(df[df['price'] <= i]))
  print('Mean =', mean)
  print('----')

"""Because there are very few outliers, the mean doesn't actually change that much."""



df.columns

"""## Bedrooms"""

df['bedrooms'].value_counts()

# small_prices = df[df['price'] <= 1500]
small_prices.plot(x='bedrooms', y='price', kind='scatter', grid=True)

# loop through each bedroom count, look at mean and median price
for i in range(11):
  subset = small_prices[small_prices['bedrooms'] == i]
  mean = subset['price'].mean()
  median = subset['price'].median()

  print('Num Bedrooms:', i)
  print('Num Datapoints:', len(subset))
  print('Mean Price:', mean)
  print('Median Price:', median)
  print('----')

"""steady increase in median/median as number of bedrooms goes up, up until about i=8. Very few datapoints for num bedrooms 6-10.

## property_type
"""

small_prices['property_type'].value_counts()

ax = small_prices.plot(x='property_type', y='price', kind='scatter', grid=True)
fig = plt.gcf()
fig.set_dpi(150)
plt.xticks(rotation=90)
sns.despine()

ax.set_title('Prices for each Property Type')

prop_types = set(small_prices['property_type'].tolist())

for prop_type in prop_types:
  subset = small_prices[small_prices['property_type'] == prop_type]
  mean = subset['price'].mean()
  median = subset['price'].median()

  print('Type:', prop_type)
  print('Num Datapoints:', len(subset))
  print('Mean Price:', mean)
  print('Median Price:', median)
  print('----')



"""## room_type


"""

df.columns

df['room_type'].value_counts()

small_prices.plot(x='room_type', y='price', kind='scatter', color='R', grid=True)

#only get those where overall satisfaction = 2
sharedRoom = df[df['room_type'] == 'Shared room']

ax = sns.displot(sharedRoom['price'])
#calc mean of all the columns and print her
mean = sharedRoom.mean()
print('\nMean\n------')
print(mean)

#only get those where overall satisfaction = 2
privateRoom = df[df['room_type'] == 'Private room']

ax = sns.displot(privateRoom['price'])
#calc mean of all the columns and print her
mean = privateRoom.mean()
print('\nMean\n------')
print(mean)

#only get those where overall satisfaction = 2
entirePlace = df[df['room_type'] == 'Entire home/apt']

ax = sns.displot(entirePlace['price'])
#calc mean of all the columns and print her
mean = entirePlace.mean()
print('\nMean\n------')
print(mean)

"""## accommodates

"""

df['accommodates'].value_counts()

small_prices.plot(x='accommodates', y='price', kind='scatter', color='R', grid=True)

#only get those where overall satisfaction = 2
accommodates1 = df[df['accommodates'] == 1]

ax = sns.displot(accommodates1['price'])
#calc mean of all the columns and print her
mean = accommodates1.mean()
print('\nMean\n------')
print(mean)

#only get those where overall satisfaction = 2
accommodates2 = df[df['accommodates'] == 2]

ax = sns.displot(accommodates2['price'])
#calc mean of all the columns and print her
mean = accommodates2.mean()
print('\nMean\n------')
print(mean)

#only get those where overall satisfaction = 2
accommodates3 = df[df['accommodates'] == 3]

ax = sns.displot(accommodates3['price'])
#calc mean of all the columns and print her
mean = accommodates3.mean()
print('\nMean\n------')
print(mean)

#only get those where overall satisfaction = 2
accommodates4 = df[df['accommodates'] == 4]

ax = sns.displot(accommodates4['price'])
#calc mean of all the columns and print her
mean = accommodates4.mean()
print('\nMean\n------')
print(mean)

#only get those where overall satisfaction = 2
accommodates5 = df[df['accommodates'] == 5]

ax = sns.displot(accommodates5['price'])
#calc mean of all the columns and print her
mean = accommodates5.mean()
print('\nMean\n------')
print(mean)

#only get those where overall satisfaction = 2
accommodates6 = df[df['accommodates'] == 6]

ax = sns.displot(accommodates6['price'])
#calc mean of all the columns and print her
mean = accommodates6.mean()
print('\nMean\n------')
print(mean)

#only get those where overall satisfaction = 2
accommodates7 = df[df['accommodates'] == 7]

ax = sns.displot(accommodates7['price'])
#calc mean of all the columns and print her
mean = accommodates7.mean()
print('\nMean\n------')
print(mean)

#only get those where overall satisfaction = 2
accommodates7 = df[df['accommodates'] == 7]

ax = sns.displot(accommodates7['price'])
#calc mean of all the columns and print her
mean = accommodates7.mean()
print('\nMean\n------')
print(mean)

#only get those where overall satisfaction = 2
accommodates8 = df[df['accommodates'] == 8]

ax = sns.displot(accommodates8['price'])
#calc mean of all the columns and print her
mean = accommodates8.mean()
print('\nMean\n------')
print(mean)

#only get those where overall satisfaction = 2
accommodates9 = df[df['accommodates'] == 9]

ax = sns.displot(accommodates9['price'])
#calc mean of all the columns and print her
mean = accommodates9.mean()
print('\nMean\n------')
print(mean)

#only get those where overall satisfaction = 2
accommodates10 = df[df['accommodates'] == 10]

ax = sns.displot(accommodates10['price'])
#calc mean of all the columns and print her
mean = accommodates10.mean()
print('\nMean\n------')
print(mean)

"""## neighborhood

"""

df['neighborhood'].value_counts()

small_prices.plot(x='neighborhood', y='price', kind='scatter', color='R', grid=True)

#only get those where overall satisfaction = 2
grandes_carrières= df[df['neighborhood'] == 'Grandes-Carrières']

ax = sns.displot(grandes_carrières['price'])
#calc mean of all the columns and print her
mean = grandes_carrières.mean()
print('\nMean\n------')
print(mean)

#only get those where overall satisfaction = 2
clignancourt= df[df['neighborhood'] == 'Clignancourt']

ax = sns.displot(clignancourt['price'])
#calc mean of all the columns and print her
mean = clignancourt.mean()
print('\nMean\n------')
print(mean)

#only get those where overall satisfaction = 2
roquette= df[df['neighborhood'] == 'Roquette']

ax = sns.displot(roquette['price'])
#calc mean of all the columns and print her
mean = roquette.mean()
print('\nMean\n------')
print(mean)

folie_méricourt= df[df['neighborhood'] == 'Folie-Méricourt']

ax = sns.displot(folie_méricourt['price'])
#calc mean of all the columns and print her
mean = folie_méricourt.mean()
print('\nMean\n------')
print(mean)

saint_lambert= df[df['neighborhood'] == 'Saint-Lambert']

ax = sns.displot(saint_lambert['price'])
#calc mean of all the columns and print her
mean = saint_lambert.mean()
print('\nMean\n------')
print(mean)

bercy= df[df['neighborhood'] == 'Bercy']

ax = sns.displot(bercy['price'])
#calc mean of all the columns and print her
mean = bercy.mean()
print('\nMean\n------')
print(mean)

invalides= df[df['neighborhood'] == 'Invalides']

ax = sns.displot(invalides['price'])
#calc mean of all the columns and print her
mean = invalides.mean()
print('\nMean\n------')
print(mean)

place_vendôme= df[df['neighborhood'] == 'Place-Vendôme']

ax = sns.displot(place_vendôme['price'])
#calc mean of all the columns and print her
mean = place_vendôme.mean()
print('\nMean\n------')
print(mean)

gaillon= df[df['neighborhood'] == 'Gaillon']

ax = sns.displot(gaillon['price'])
#calc mean of all the columns and print her
mean = gaillon.mean()
print('\nMean\n------')
print(mean)

st_germain_lAuxerrois= df[df['neighborhood'] == 'St-Germain-lAuxerrois']

ax = sns.displot(st_germain_lAuxerrois['price'])
#calc mean of all the columns and print her
mean = st_germain_lAuxerrois.mean()
print('\nMean\n------')
print(mean)

"""## overall_satisfaction

"""

small_prices.plot(x='overall_satisfaction', y='price', kind='scatter', color='R', grid=True)

df['overall_satisfaction'].value_counts()

#only get those where overall satisfaction = 1
satisfaction_1 = df[df['overall_satisfaction'] == 1]
ax = sns.displot(satisfaction_1['price'])
#calc mean of all the columns and print her
mean = satisfaction_1.mean()
print('\nMean\n------')
print(mean)



#only get those where overall satisfaction = 2
satisfaction_2 = df[df['overall_satisfaction'] == 2]
ax = sns.displot(satisfaction_2['price'])
#calc mean of all the columns and print her
mean = satisfaction_2.mean()
print('\nMean\n------')
print(mean)

#only get those where overall satisfaction = 3
satisfaction_3 = df[df['overall_satisfaction'] == 3]
ax = sns.displot(satisfaction_3['price'])
#calc mean of all the columns and print her
mean = satisfaction_3.mean()
print('\nMean\n------')
print(mean)



#only get those where overall satisfaction = 4
satisfaction_4 = df[df['overall_satisfaction'] == 4]
ax = sns.displot(satisfaction_4['price'])
#calc mean of all the columns and print her
mean = satisfaction_4.mean()
print('\nMean\n------')
print(mean)



#only get those where overall satisfaction = 5
satisfaction_5 = df[df['overall_satisfaction'] == 5]
ax = sns.displot(satisfaction_5['price'])
#calc mean of all the columns and print her
mean = satisfaction_5.mean()
print('\nMean\n------')
print(mean)

"""# Data Cleanup/Setup 
- use the outlier cuts we decided using EDA
- convert to list of dictionaries? to_dict('records') 
- shuffle and split into training and validation sets
"""

# remove price outliers >1500
df = df[df['price'] <= 1500]
print(len(df))

df['log_price'] = np.log(df['price'])

global_price_mean = df['price'].mean()
global_log_price_mean = df['log_price'].mean()
print('Global price mean:', global_price_mean)
print('Global log price mean:', global_log_price_mean)

data = df.to_dict('records')
df['price'].describe()

random.shuffle(data)
y = [d['price'] for d in data]

# splits
split_idx = len(df) - 10000
train_set = data[:split_idx]
val_set = data[split_idx:]
train_y = y[:split_idx]
val_y = y[split_idx:]

# set up normalizing factors
max_train_price = float('-inf')
min_train_price = float('inf')
max_val_price = float('-inf')
min_val_price = float('inf')

for datum in train_set:
  max_train_price = max(max_train_price, datum['price'])
  min_train_price = min(min_train_price, datum['price'])

for datum in val_set:
  max_val_price = max(max_val_price, datum['price'])
  min_val_price = min(min_val_price, datum['price'])

min_val_price, min_train_price, max_val_price, max_train_price

data[0]

"""# Baseline: property_type
- Simple model using one feature and conditionals for prediction.
"""

# baseline model using conditionals
# predicts the mean price of each property type
def baseline(data):
  yPred = []
  
  # predict mean price of each category
  for d in data:
    if d['property_type'] == 'Apartment':
      yPred.append(np.log(107.538))
    elif d['property_type'] == 'Loft':
      yPred.append(np.log(166.091))
    elif d['property_type'] == 'House':
      yPred.append(np.log(184.345))
    elif d['property_type'] == 'Condominium':
      yPred.append(np.log(92.217))
    elif d['property_type'] == 'Bed & Breakfast':
      yPred.append(np.log(122.036))
    elif d['property_type'] == 'Other':
      yPred.append(np.log(111.401))
    elif d['property_type'] == 'Guesthouse':
      yPred.append(np.log(75.192))
    elif d['property_type'] == 'Boutique hotel':
      yPred.append(np.log(148.919))
    elif d['property_type'] == 'Townhouse':
      yPred.append(np.log(220.220))
    else:
      yPred.append(global_log_price_mean)

  return yPred

# evaluate on validation set
yPred = baseline(val_set)

MSE = metrics.mean_squared_error(val_y, yPred)
print('MSE:', MSE)

metrics.explained_variance_score(val_y, yPred)

"""## RMSE on df['price']"""

# baseline model using conditionals
# predicts the mean price of each property type
def baseline(data):
  yPred = []
  
  # predict mean price of each category
  for d in data:
    if d['property_type'] == 'Apartment':
      yPred.append(107.538)
    elif d['property_type'] == 'Loft':
      yPred.append(166.091)
    elif d['property_type'] == 'House':
      yPred.append(184.345)
    elif d['property_type'] == 'Condominium':
      yPred.append(92.217)
    elif d['property_type'] == 'Bed & Breakfast':
      yPred.append(122.036)
    elif d['property_type'] == 'Other':
      yPred.append(111.401)
    elif d['property_type'] == 'Guesthouse':
      yPred.append(75.192)
    elif d['property_type'] == 'Boutique hotel':
      yPred.append(148.919)
    elif d['property_type'] == 'Townhouse':
      yPred.append(220.220)
    else:
      yPred.append(global_price_mean)

  return yPred

# evaluate on validation set
yPred = baseline(val_set)

RMSE = metrics.mean_squared_error(val_y, yPred, squared = False)
print('RMSE:', RMSE)
print('Norm RMSE:', RMSE/(max_val_price - min_val_price))

metrics.explained_variance_score(val_y, yPred)



"""# Univariate Models

## Model: room_type

### Conditionals implementation
"""

def baseline1_cond(data):
  yPred = []

  for d in data:
    if d['room_type'] == 'Entire home/apt':
      yPred.append(np.log(117.159))
    elif d['room_type'] == 'Private room':
      yPred.append(np.log(64.924))
    elif d['room_type'] == 'Shared room':
      yPred.append(np.log(45.359))

  return yPred

# evaluate on validation set
yPred = baseline1_cond(val_set)

MSE = metrics.mean_squared_error(val_y, yPred)
print('MSE:', MSE)

"""### One-hot encoding implementation"""

# One hot encoding + linear reg implementation
categoryCounts = defaultdict(int)
for d in data:
  categoryCounts[d['room_type']] += 1

# tbh we dont need to do the categories for this one lmao but whatever
categories = [c for c in categoryCounts]
catID = dict(zip(list(categories), range(len(categories))))
catID

one_hot = []
for d in data.copy():
  temp = [0] * len(categories)
  if d['room_type'] in categories:
    idx = catID[d['room_type']]
    temp[idx] = 1
  one_hot.append(temp)
one_hot[:10]

# split one_hot into training and val sets
train_X = one_hot[:split_idx]
val_X = one_hot[split_idx:]

len(train_X), len(val_X)

mod = linear_model.LinearRegression(fit_intercept = True)
mod.fit(train_X, train_y)

yPred = mod.predict(val_X)
RMSE = metrics.mean_squared_error(val_y, yPred, squared = False)
print('RMSE:', RMSE)
print('Norm RMSE:', RMSE/(max_val_price - min_val_price))

"""## Model: accommodates

### One-hot encoding implementation
"""

# One hot encoding + linear reg implementation
categoryCounts = defaultdict(int)
for d in data:
  categoryCounts[d['accommodates']] += 1

# TODO: experiment with this cutoff?
categories = [c for c in categoryCounts if categoryCounts[c] >= 25]
catID = dict(zip(list(categories), range(len(categories))))
catID

one_hot = []
for d in data.copy():
  temp = [0] * len(categories)
  if d['accommodates'] in categories:
    idx = catID[d['accommodates']]
    temp[idx] = 1
  one_hot.append(temp)
one_hot[:10]

# split one_hot into training and val sets
train_X = one_hot[:split_idx]
val_X = one_hot[split_idx:]

len(train_X), len(val_X)

mod = linear_model.LinearRegression(fit_intercept = True)
mod.fit(train_X, train_y)

yPred = mod.predict(val_X)
RMSE = metrics.mean_squared_error(val_y, yPred, squared = False)
print('RMSE:', RMSE)
print('Norm RMSE:', RMSE/(max_val_price - min_val_price))

"""## Model: property_type

### One-hot encoding implementation
"""

# One hot encoding + linear reg implementation
categoryCounts = defaultdict(int)
for d in data:
  categoryCounts[d['property_type']] += 1

# TODO: experiment with this cutoff?
categories = [c for c in categoryCounts if categoryCounts[c] >= 25]
catID = dict(zip(list(categories), range(len(categories))))
catID

one_hot = []
for d in data.copy():
  temp = [0] * len(categories)
  if d['property_type'] in categories:
    idx = catID[d['property_type']]
    temp[idx] = 1
  one_hot.append(temp)
one_hot[:10]

"""This behavior is expected because 66k of our total number of datapoints are of the property type "Apartment", which has an index of 0 in our one-hot encoding."""

# split one_hot into training and val sets
train_X = one_hot[:split_idx]
val_X = one_hot[split_idx:]

len(train_X), len(val_X)

mod = linear_model.LinearRegression(fit_intercept = True)
mod.fit(train_X, train_y)

yPred = mod.predict(val_X)
RMSE = metrics.mean_squared_error(val_y, yPred, squared = False)
print('RMSE:', RMSE)
print('Norm RMSE:', RMSE/(max_val_price - min_val_price))

val_y[:10]

yPred[:10]



"""## TODO Model: neighborhood"""

categoryCounts = defaultdict(int)
for d in train_set:
  categoryCounts[d['neighborhood']] += 1

# TODO: experiment with this cutoff? 
neighbor_categories = [c for c in categoryCounts if categoryCounts[c] >= 25]
neighborID = dict(zip(list(neighbor_categories), range(len(neighbor_categories))))

"""## TODO Model: bedrooms"""

categoryCounts = defaultdict(int)
for d in train_set:
  categoryCounts[d['bedrooms']] += 1

# TODO: experiment with this cutoff? 
bedroom_categories = [c for c in categoryCounts if categoryCounts[c] >= 25]
bedroomID = dict(zip(list(bedroom_categories), range(len(bedroom_categories))))

"""# Multivariate Models
- models that incorporate more than 1 feature when predicting

## Model: room_type & accommodates

Feature vector consists of one-hot encoding for both room_type and accommodates
"""

# set up indexing dictionaries
roomID = {'Entire home/apt': 0, 'Private room': 1, 'Shared room': 2}

categoryCounts = defaultdict(int)
for d in train_set:
  categoryCounts[d['accommodates']] += 1

# TODO: experiment with this cutoff? 
acc_categories = [c for c in categoryCounts if categoryCounts[c] >= 25]
accID = dict(zip(list(acc_categories), range(len(acc_categories))))


# one-hot encodings for 2 variables
def feature(datum):
  room_feat = [0] * len(roomID)
  acc_feat = [0] * len(accID)

  room_feat[roomID[datum['room_type']]] = 1

  if datum['accommodates'] in acc_categories:
    idx = accID[datum['accommodates']]
    acc_feat[idx] = 1

  return room_feat + acc_feat


# set up datasets
train_X = [feature(d) for d in train_set]
val_X = [feature(d) for d in val_set]
# train_y and val_y defined in data setup

mod = linear_model.LinearRegression(fit_intercept = True)
mod.fit(train_X, train_y)
yPred = mod.predict(val_X)

RMSE = metrics.mean_squared_error(val_y, yPred, squared = False)
print('RMSE:', RMSE)
print('Norm RMSE:', RMSE/(max_val_price - min_val_price))

"""For this particular run/split of data, this bivariate model achieves 0.20376707263098282 while the accommodates model achieves 0.21047275236486027. Slight improvement

## Model: property_type & accommodates
"""

# set up categories for property_type, reuse acc_ID and acc_categories
categoryCounts = defaultdict(int)
for d in train_set:
  categoryCounts[d['property_type']] += 1

# TODO: experiment with this cutoff? 
prop_categories = [c for c in categoryCounts if categoryCounts[c] >= 25]
propID = dict(zip(list(prop_categories), range(len(prop_categories))))


# one-hot encodings for 2 variables
def feature(datum):
  prop_feat = [0] * len(propID)
  acc_feat = [0] * len(accID)

  if datum['property_type'] in prop_categories:
    idx = propID[datum['property_type']]
    prop_feat[idx] = 1

  if datum['accommodates'] in acc_categories:
    idx = accID[datum['accommodates']]
    acc_feat[idx] = 1

  return prop_feat + acc_feat

# set up datasets
train_X = [feature(d) for d in train_set]
val_X = [feature(d) for d in val_set]
# train_y and val_y defined in data setup

mod = linear_model.LinearRegression(fit_intercept = True)
mod.fit(train_X, train_y)
yPred = mod.predict(val_X)

RMSE = metrics.mean_squared_error(val_y, yPred, squared = False)
print('RMSE:', RMSE)
print('Norm RMSE:', RMSE/(max_val_price - min_val_price))

"""This MSE, 0.20840952509499333, performs slightly worse than the bivariate model with room_type and accommodates, which has an MSE of 0.20376707263098282.

## Model: property_type & room_type
"""

def feature(datum):
  prop_feat = [0] * len(propID)
  room_feat = [0] * len(roomID)

  if datum['property_type'] in prop_categories:
    idx = propID[datum['property_type']]
    prop_feat[idx] = 1

  room_feat[roomID[datum['room_type']]] = 1

  return prop_feat + room_feat

# set up datasets
train_X = [feature(d) for d in train_set]
val_X = [feature(d) for d in val_set]
# train_y and val_y defined in data setup

mod = linear_model.LinearRegression(fit_intercept = True)
mod.fit(train_X, train_y)
yPred = mod.predict(val_X)

RMSE = metrics.mean_squared_error(val_y, yPred, squared = False)
print('RMSE:', RMSE)
print('Norm RMSE:', RMSE/(max_val_price - min_val_price))

"""This model (MSE = 0.2842062562206476) performs significantly worse than the previous 2 models, which both had MSEs of about 0.20.

## Model: property_type, room_type, & accommodates
"""

def feature(datum):
  prop_feat = [0] * len(propID)
  room_feat = [0] * len(roomID)
  acc_feat = [0] * len(accID)

  if datum['property_type'] in prop_categories:
    idx = propID[datum['property_type']]
    prop_feat[idx] = 1

  room_feat[roomID[datum['room_type']]] = 1

  if datum['accommodates'] in acc_categories:
    idx = accID[datum['accommodates']]
    acc_feat[idx] = 1

  return prop_feat + room_feat + acc_feat

# set up datasets
train_X = [feature(d) for d in train_set]
val_X = [feature(d) for d in val_set]
# train_y and val_y defined in data setup

mod = linear_model.LinearRegression(fit_intercept = True)
mod.fit(train_X, train_y)
yPred = mod.predict(val_X)

RMSE = metrics.mean_squared_error(val_y, yPred, squared = False)
print('RMSE:', RMSE)
print('Norm RMSE:', RMSE/(max_val_price - min_val_price))

"""performs even better than the model with room_type and accommodates

0.20060486360623153 < 0.20376707263098282

# Random Forest Model
"""

# utilizes sklearn.ensemble.RandomForestRegressor class
list(data[0].keys())

def random_forest(features, num_estimators, val_set):
  # TRAINING SET SETUP
  train_X = []
  for datum in train_set:
    feat = []
    if 'room_type' in features:
      room_feat = [0] * len(roomID)
      room_feat[roomID[datum['room_type']]] = 1
      feat = feat + room_feat

    if 'neighborhood' in features:
      neigh_feat = [0] * len(neighborID)
      if datum['neighborhood'] in neighbor_categories:
        idx = neighborID[datum['neighborhood']] 
        neigh_feat[idx] = 1
      feat = feat + neigh_feat
    if 'accommodates' in features:
      acc_feat = [0] * len(accID)
      if datum['accommodates'] in acc_categories:
        idx = accID[datum['accommodates']] 
        acc_feat[idx] = 1
      feat = feat + acc_feat

    if 'bedrooms' in features:
      bedroom_feat = [0] * len(bedroomID)
      if datum['bedrooms'] in bedroom_categories:
        idx = bedroomID[datum['bedrooms']] 
        bedroom_feat[idx] = 1
      feat = feat + bedroom_feat

    if 'property_type' in features:
      prop_feat = [0] * len(propID)
      if datum['property_type'] in prop_categories:
        idx = propID[datum['property_type']] 
        prop_feat[idx] = 1
      feat = feat + prop_feat
    
    train_X.append(feat)

  # VAL SET SETUP
  val_X = []
  for datum in val_set:
    feat = []
    if 'room_type' in features:
      room_feat = [0] * len(roomID)
      room_feat[roomID[datum['room_type']]] = 1
      feat = feat + room_feat

    if 'neighborhood' in features:
      neigh_feat = [0] * len(neighborID)
      if datum['neighborhood'] in neighbor_categories:
        idx = neighborID[datum['neighborhood']] 
        neigh_feat[idx] = 1
      feat = feat + neigh_feat

    if 'accommodates' in features:
      acc_feat = [0] * len(accID)
      if datum['accommodates'] in acc_categories:
        idx = accID[datum['accommodates']] 
        acc_feat[idx] = 1
      feat = feat + acc_feat

    if 'bedrooms' in features:
      bedroom_feat = [0] * len(bedroomID)
      if datum['bedrooms'] in bedroom_categories:
        idx = bedroomID[datum['bedrooms']] 
        bedroom_feat[idx] = 1
      feat = feat + bedroom_feat

    if 'property_type' in features:
      prop_feat = [0] * len(propID)
      if datum['property_type'] in prop_categories:
        idx = propID[datum['property_type']] 
        prop_feat[idx] = 1
      feat = feat + prop_feat
    
    val_X.append(feat)
    
  # Random Forest Regressor
  forest = RandomForestRegressor(n_estimators=num_estimators, 
                                  max_depth=None,
                                  max_features='auto',
                                  max_samples=0.4)
  forest.fit(train_X, train_y)

  train_yPred = forest.predict(train_X)
  train_RMSE = metrics.mean_squared_error(train_y, train_yPred, squared = False)
  print('Norm RMSE:', train_RMSE/(max_train_price - min_train_price))

  val_yPred = forest.predict(val_X)
  val_RMSE = metrics.mean_squared_error(val_y, val_yPred, squared = False)
  print('Norm RMSE:', val_RMSE/(max_val_price - min_val_price))

  #train_EVS = metrics.explained_variance_score(train_y, train_yPred)
  #val_EVS = metrics.explained_variance_score(val_y, val_yPred)

  print(len(train_X[0]), len(val_X[0]))
  #print('R^2 scores:', forest.score(train_X, train_y), forest.score(val_X, val_y))

  fig, ax = plt.subplots()
  sns.kdeplot(val_y, ax = ax)
  sns.kdeplot(val_yPred, ax = ax)

  

  #return (MSE, forest.score(train_X, train_y), forest.score(val_X, val_y))
  return train_RMSE, val_RMSE

# for reference, default length for all features in the one-hot encoding
len(roomID) + len(neighborID) + len(accID) + len(bedroomID) + len(propID)

features = ['room_type', 'neighborhood', 'accommodates', 'bedrooms', 'property_type']

"""found that 50 is the best n_estimators by a small margin, so we'll just roll with it. next, try different sets of features"""

features = ['room_type', 'neighborhood', 'accommodates', 'bedrooms', 'property_type']

'''
# try single features for comparison
best_MSE = float('inf')
best_feat = ''

for feature in features:
  MSE = random_forest(feature, 50, val_set)
  if MSE < best_MSE:
    best_MSE = MSE
    best_feat = feature
  print('Feature:', feature)
  print('MSE:', MSE)
  print('----')
  
print('Best MSE:', best_MSE)
print('Best Feature:', best_feat)
'''

# only use accommodates and bedrooms, the best performing features
random_forest(['accommodates', 'bedrooms'], 50, val_set)

"""still worse than the RF trained on all the features

Based on tests with subsets of the features, seems like best RF model is trained on all the features, and only small differences with number of estimators
"""

features = ['room_type', 'neighborhood', 'accommodates', 'bedrooms', 'property_type']
random_forest(features, 50, val_set)

features = ['room_type', 'accommodates', 'property_type']
random_forest(features, 50, val_set)

"""max_depth=None\
train_MSE, val_MSE\
(0.12388116750525195, 0.14075701739209404)

max_depth=50\
train_MSE, val_MSE\
(0.12564969006998963, 0.14123857220518402)

# Ridge Regression
"""

def ridge_regression(features, val_set, alpha = 1.0):
  # TRAINING SET SETUP
  train_X = []
  for datum in train_set:
    feat = []
    if 'room_type' in features:
      room_feat = [0] * len(roomID)
      room_feat[roomID[datum['room_type']]] = 1
      feat = feat + room_feat

    if 'neighborhood' in features:
      neigh_feat = [0] * len(neighborID)
      if datum['neighborhood'] in neighbor_categories:
        idx = neighborID[datum['neighborhood']] 
        neigh_feat[idx] = 1
      feat = feat + neigh_feat
    if 'accommodates' in features:
      acc_feat = [0] * len(accID)
      if datum['accommodates'] in acc_categories:
        idx = accID[datum['accommodates']] 
        acc_feat[idx] = 1
      feat = feat + acc_feat

    if 'bedrooms' in features:
      bedroom_feat = [0] * len(bedroomID)
      if datum['bedrooms'] in bedroom_categories:
        idx = bedroomID[datum['bedrooms']] 
        bedroom_feat[idx] = 1
      feat = feat + bedroom_feat

    if 'property_type' in features:
      prop_feat = [0] * len(propID)
      if datum['property_type'] in prop_categories:
        idx = propID[datum['property_type']] 
        prop_feat[idx] = 1
      feat = feat + prop_feat
    
    train_X.append(feat)

  # VAL SET SETUP
  val_X = []
  for datum in val_set:
    feat = []
    if 'room_type' in features:
      room_feat = [0] * len(roomID)
      room_feat[roomID[datum['room_type']]] = 1
      feat = feat + room_feat

    if 'neighborhood' in features:
      neigh_feat = [0] * len(neighborID)
      if datum['neighborhood'] in neighbor_categories:
        idx = neighborID[datum['neighborhood']] 
        neigh_feat[idx] = 1
      feat = feat + neigh_feat

    if 'accommodates' in features:
      acc_feat = [0] * len(accID)
      if datum['accommodates'] in acc_categories:
        idx = accID[datum['accommodates']] 
        acc_feat[idx] = 1
      feat = feat + acc_feat

    if 'bedrooms' in features:
      bedroom_feat = [0] * len(bedroomID)
      if datum['bedrooms'] in bedroom_categories:
        idx = bedroomID[datum['bedrooms']] 
        bedroom_feat[idx] = 1
      feat = feat + bedroom_feat

    if 'property_type' in features:
      prop_feat = [0] * len(propID)
      if datum['property_type'] in prop_categories:
        idx = propID[datum['property_type']] 
        prop_feat[idx] = 1
      feat = feat + prop_feat
    
    val_X.append(feat)
    
  # Ridge Regression
  clf = linear_model.Ridge(alpha = alpha, fit_intercept=True)
  clf.fit(train_X, train_y)

  train_yPred = clf.predict(train_X)
  train_RMSE = metrics.mean_squared_error(train_y, train_yPred, squared = False)
  print('Norm RMSE:', train_RMSE/(max_train_price - min_train_price))


  val_yPred = clf.predict(val_X)
  val_RMSE = metrics.mean_squared_error(val_y, val_yPred, squared = False)
  print('Norm RMSE:', val_RMSE/(max_val_price - min_val_price))


  print(len(train_X[0]), len(val_X[0]))

  fig, ax = plt.subplots()
  sns.kdeplot(val_y, ax = ax)
  sns.kdeplot(val_yPred, ax = ax)

  #return (MSE, forest.score(train_X, train_y), forest.score(val_X, val_y))
  return train_RMSE, val_RMSE

features = ['room_type', 'neighborhood', 'accommodates', 'bedrooms', 'property_type']
ridge_regression(features, val_set)

features = ['room_type', 'accommodates', 'property_type']
ridge_regression(features, val_set)

'''
for alpha in [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:
  print('alpha:', alpha)
  print(ridge_regression(features, val_set, alpha))
  print('----')
'''

"""# Lasso Regression"""



"""# SVM """

def support_vec_reg(features, val_set):
  # TRAINING SET SETUP
  train_X = []
  for datum in train_set:
    feat = []
    if 'room_type' in features:
      room_feat = [0] * len(roomID)
      room_feat[roomID[datum['room_type']]] = 1
      feat = feat + room_feat

    if 'neighborhood' in features:
      neigh_feat = [0] * len(neighborID)
      if datum['neighborhood'] in neighbor_categories:
        idx = neighborID[datum['neighborhood']] 
        neigh_feat[idx] = 1
      feat = feat + neigh_feat
    if 'accommodates' in features:
      acc_feat = [0] * len(accID)
      if datum['accommodates'] in acc_categories:
        idx = accID[datum['accommodates']] 
        acc_feat[idx] = 1
      feat = feat + acc_feat

    if 'bedrooms' in features:
      bedroom_feat = [0] * len(bedroomID)
      if datum['bedrooms'] in bedroom_categories:
        idx = bedroomID[datum['bedrooms']] 
        bedroom_feat[idx] = 1
      feat = feat + bedroom_feat

    if 'property_type' in features:
      prop_feat = [0] * len(propID)
      if datum['property_type'] in prop_categories:
        idx = propID[datum['property_type']] 
        prop_feat[idx] = 1
      feat = feat + prop_feat
    
    train_X.append(feat)

  # VAL SET SETUP
  val_X = []
  for datum in val_set:
    feat = []
    if 'room_type' in features:
      room_feat = [0] * len(roomID)
      room_feat[roomID[datum['room_type']]] = 1
      feat = feat + room_feat

    if 'neighborhood' in features:
      neigh_feat = [0] * len(neighborID)
      if datum['neighborhood'] in neighbor_categories:
        idx = neighborID[datum['neighborhood']] 
        neigh_feat[idx] = 1
      feat = feat + neigh_feat

    if 'accommodates' in features:
      acc_feat = [0] * len(accID)
      if datum['accommodates'] in acc_categories:
        idx = accID[datum['accommodates']] 
        acc_feat[idx] = 1
      feat = feat + acc_feat

    if 'bedrooms' in features:
      bedroom_feat = [0] * len(bedroomID)
      if datum['bedrooms'] in bedroom_categories:
        idx = bedroomID[datum['bedrooms']] 
        bedroom_feat[idx] = 1
      feat = feat + bedroom_feat

    if 'property_type' in features:
      prop_feat = [0] * len(propID)
      if datum['property_type'] in prop_categories:
        idx = propID[datum['property_type']] 
        prop_feat[idx] = 1
      feat = feat + prop_feat
    
    val_X.append(feat)

  # create and fit 3 regression models
  svr_lin = SVR(kernel = 'linear')
  svr_poly = SVR(kernel = 'poly', degree = 2)
  svr_rbf = SVR(kernel = 'rbf', gamma = 0.1)

  svr_lin.fit(train_X, train_y)
  svr_poly.fit(train_X, train_y)
  svr_rbf.fit(train_X, train_y)

  # predict and calculate RMSE
  linRMSE = metrics.mean_squared_error(val_y, svr_lin.predict(val_X)[0], squared = False)
  polyRMSE = metrics.mean_squared_error(val_y, svr_poly.predict(val_X)[0], squared = False)
  rbfRMSE = metrics.mean_squared_error(val_y, svr_rbf.predict(val_X)[0], squared = False)

  return linRMSE, polyRMSE, rbfRMSE

#features = ['room_type', 'neighborhood', 'accommodates', 'bedrooms', 'property_type']
#support_vec_reg(features, val_set)

